# 資料預處理筆記

## 目錄

- [資料預處理筆記](#資料預處理筆記)
  - [目錄](#目錄)
  - [類別資料編碼](#類別資料編碼)
    - [Label encoding](#label-encoding)
    - [One hot encoding](#one-hot-encoding)
    - [使用場合](#使用場合)
  - [特徵縮放](#特徵縮放)
    - [Normalization (歸一化)](#normalization-歸一化)
    - [Standardization (標準化)](#standardization-標準化)
  - [資料處理步驟](#資料處理步驟)
    - [處理缺失值 (Missing Data)](#處理缺失值-missing-data)
    - [數據集分割 (Train-Test Split)](#數據集分割-train-test-split)
    - [特徵縮放注意事項](#特徵縮放注意事項)
  - [總結](#總結)

## 類別資料編碼

### Label encoding

- **定義**：將類別變量轉換為數字標籤。
- **實現方式**：每個類別都分配一個唯一的數字。
- **例子**：還是用顏色變量，包含三種顏色：紅色、綠色和藍色。
  - 原始數據：`['紅色', '綠色', '藍色', '紅色']`
  - Label 編碼後：（假設：紅色 = 0，綠色 = 1，藍色 = 2）
    ```csharp
    [0, 1, 2, 0]
    ```
- **優點**：
  - 簡單且不會增加特徵數量。
  - 適合有順序關係的類別數據（如：低、中、高）。
- **缺點**：
  - 引入類別之間的排序或大小比較，可能會影響模型的性能。

### One hot encoding

- **定義**：將類別變量轉換為二進位變量。
- **實現方式**：對每個類別創建一個新的二進位欄位，並使用 0 或 1 表示該類別是否出現在觀測值中。
- **例子**：假設我們有一個顏色變量，包含三種顏色：紅色、綠色和藍色。
  - 原始數據：`['紅色', '綠色', '藍色', '紅色']`
  - One-Hot 編碼後：
    ```
    Copy code
    紅色 綠色 藍色
    1    0    0
    0    1    0
    0    0    1
    1    0    0
    ```
- **優點**：
  - 不引入類別之間的排序或大小比較。
  - 適合用於沒有內在順序的類別數據。
- **缺點**：
  - 當類別數量很多時，會引入大量的特徵，增加計算資源的消耗。
- **應用** : 為每個類別新增一個欄位，用 0/1 表示是否
- **使用時機**：
  - 當類別特徵的值之間沒有順序或大小的關係時，應使用一位有效編碼。這避免了機器學習模型錯誤學習到類別之間的數值大小關係，如顏色、地點類別。
  - 線性模型（如線性回歸、邏輯回歸）通常需要使用一位有效編碼，因為這些模型會受到數據的數值影響。

### 使用場合

- **One-Hot Encoding**：適合用於沒有內在順序的類別變量，如顏色、地點、產品類別等。
- **Label Encoding**：適合用於有內在順序的類別變量，如排名、等級等。

這兩種編碼技術各有優缺點，選擇哪種編碼方式需要根據具體的數據和模型需求來決定

## 特徵縮放

### Normalization (歸一化)

歸一化通常是指將數據按比例縮放，使之落入一個小的特定區間，常見的是[0, 1]區間。在某些算法中，如使用梯度下降的算法，如果數據的範圍很廣，會導致梯度下降非常緩慢。歸一化的一種常用方法是最小-最大縮放

**定義**：將數據縮放到一個固定的範圍，通常是 [0, 1] 或 [-1, 1]。

x - 減掉最小值再除上全部距離，將可把值範圍調整到 0~1 之間

**目的**：主要用於確保不同特徵具有相同的量級，以便某些距離度量方法（如歐氏距離）在計算時不會被某些量級較大的特徵主導。

**方法**：

- **最小-最大縮放（Min-Max Scaling）**：將數據縮放到 [0, 1] 範圍內。
  - 公式：
    $$
    X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
    $$
  - 其中，X 是原始數據，Xmin 和 Xmax 分別是數據的最小值和最大值。

**例子**：
假設我們有以下數據： $[2 , 4, 6 , 8 , 10]$

- 計算最小值 $X_{min}=2$
- 計算最大值 $X_{max}=10$
- 歸一化後的數據：

$$
\left[ \frac{2-2}{10-2}, \frac{4-2}{10-2}, \frac{6-2}{10-2}, \frac{8-2}{10-2}, \frac{10-2}{10-2} \right] = [0, 0.25, 0.5, 0.75, 1]
$$

### Standardization (標準化)

**定義**：將數據轉換為均值為 0，標準差為 1 的數據分佈，即將數據轉換為標準正態分佈。

**目的**：主要用於確保不同特徵具有相同的分佈特徵（均值和標準差），這在使用某些機器學習算法（如線性回歸、支持向量機等）時特別重要。

**方法**：

- **Z-Score 標準化**：將數據轉換為標準正態分佈。
  - 公式：$X_{std} = \frac{X - \mu}{\sigma}$
  - 其中，$\mu$ 是數據的均值，$\sigma$ 是數據的標準差。

假設我們有以下數據：$[2,4,6,8,10]$

- 計算均值：$\mu = 6$
- 計算標準差：$\sigma \approx 3.16$

$$
s = \sqrt{\frac{(2 - 6)^2 + (4 - 6)^2 + (6 - 6)^2 + (8 - 6)^2 + (10 - 6)^2}{5 - 1}}
$$

$$
s = \sqrt{\frac{16 + 4 + 0 + 4 + 16}{4}} = \sqrt{\frac{40}{4}} = \sqrt{10} \approx 3.16
$$

x = 2 時，$X_{std} = \frac{2 - 6}{3.16} \approx -1.26$

x = 4 時，$X_{std} = \frac{4 - 6}{3.16} \approx -0.63$

x = 6 時，$X_{std} = \frac{6 - 6}{3.16} \approx 0$

x = 8 時，$X_{std} = \frac{8 - 6}{3.16} \approx 0.63$

x = 10 時，$X_{std} = \frac{10 - 6}{3.16} \approx 1.26$

標準化後的數據：

$$
[-1.26, -0.63, 0, 0.63, 1.26]
$$

## 資料處理步驟

### 處理缺失值 (Missing Data)

- **使用 SimpleImputer**：
  ```python
  from sklearn.impute import SimpleImputer
  imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
  ```
- **常用策略**：
  - `mean`：使用非缺失值的平均值
  - `median`：使用非缺失值的中位數
  - `most_frequent`：使用最常見的值
  - `constant`：使用指定的常數值
- **步驟**：
  1. `fit()`：計算替換值（如平均值）
  2. `transform()`：將計算出的值替換到實際數據中

### 數據集分割 (Train-Test Split)

- **目的**：將數據集分為訓練集和測試集，以評估模型的泛化能力
- **實現**：
  ```python
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=1
  )
  ```
- **重要參數**：
  - `test_size`：測試集的比例（常用 0.2-0.3）
  - `random_state`：隨機種子，確保結果可重現
  - `stratify`：用於分層抽樣，確保分割後的數據集保持原始類別分布

### 特徵縮放注意事項

- **時機選擇**：
  - 必須在數據分割之後進行
  - 對測試集使用與訓練集相同的縮放參數
- **OneHot 編碼後的處理**：
  - 不需要對 OneHot 編碼後的特徵進行縮放
  - 僅對數值型特徵進行縮放
- **程式碼範例**：
  ```python
  from sklearn.preprocessing import StandardScaler
  sc = StandardScaler()
  X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])  # 僅對數值特徵縮放
  X_test[:, 3:] = sc.transform(X_test[:, 3:])  # 使用訓練集的參數
  ```

## 總結

- **類別資料編碼**：

  - Label Encoding 適合有序類別
  - One-Hot Encoding 適合無序類別
  - 選擇方式取決於數據特性和模型需求

- **特徵縮放**：

  - 歸一化：將數據縮放到固定範圍，適合距離度量方法
  - 標準化：轉換為均值為 0，標準差為 1 的分佈，適合線性模型
  - 需要在數據分割後進行，且測試集使用訓練集的參數

- **處理流程重點**：
  1. 先處理缺失值
  2. 進行類別編碼
  3. 分割訓練集和測試集
  4. 最後進行特徵縮放

# Multiple Linear Regression

## 目錄

1. [什麼是 Multiple Linear Regression?](#什麼是-multiple-linear-regression)
   - [數學表示](#數學表示)
2. [應用場景](#應用場景)
3. [多重共線性陷阱](#多重共線性陷阱)
   - [完全共線性](#完全共線性dummy-variable-trap)
   - [高共線性](#高共線性high-multicollinearity)
4. [建立模型](#建立模型)
   - [All-in](#1-all-in)
   - [Backward Elimination](#2-backward-elimination)
   - [Forward Selection](#3-forward-selection)
   - [Bidirectional Elimination](#4-bidirectional-elimination)
5. [特徵縮放](#特徵縮放)

## 什麼是 Multiple Linear Regression?

Multiple Linear Regression（多元線性回歸）是一種統計分析方法，用於建立多個自變量（特徵）與一個因變量（目標）之間的線性關係。與簡單線性回歸不同，多元線性回歸可以同時考慮多個特徵對目標變量的影響。

### 數學表示

多元線性回歸的一般形式為：

$$
Y = β_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ε
$$

其中：

- Y 是因變量（預測目標）
- X₁, X₂, ..., Xₙ 是自變量（特徵）
- β₀ 是截距
- β₁, β₂, ..., βₙ 是回歸係數
- ε 是誤差項

## 應用場景

多元線性回歸在許多領域都有廣泛應用：

1. **房地產價格預測**

   - 使用面積、位置、房間數量等多個特徵預測房屋價格

2. **銷售預測**

   - 根據廣告支出、季節、競爭對手價格等因素預測銷售量

3. **金融分析**

   - 分析多個經濟指標對股票價格的影響

4. **醫療研究**
   - 研究多個健康指標對疾病風險的影響

## 多重共線性陷阱

在多元線性回歸中，使用虛擬變量（dummy variables）時會遇到的常見問題稱為**多重共線性陷阱**。這種陷阱主要有以下兩種形式：

### 完全共線性（Dummy Variable Trap）

- 這種情況發生在我們為一個分類變量（categorical variable）創建虛擬變量時，包含了所有可能的分類（categories）的虛擬變量。
- 為了避免這個問題，我們需要避免完全共線性，即對於一個有 k 個類別的分類變量，只創建 k−1 個虛擬變量。

#### 錯誤示範：完全共線性陷阱

我們為地點這個變量創建了三個虛擬變量：

- $D_A = 1$ (如果地點是城市 A，否則為 0）
- $D_B = 1$ (如果地點是城市 B，否則為 0）
- $D_C = 1$ (如果地點是城市 C，否則為 0）

這時，我們的回歸模型可能看起來像這樣：

$$
Price=β_0+β_1⋅D_A+β_2⋅D_B+β_3⋅D_C+ϵ
$$

#### 正確示範：避免完全共線性陷阱

選擇城市 C 作為基準類別，只創建兩個虛擬變量：

- $D_A = 1$ (如果地點是城市 A，否則為 0）
- $D_B = 1$ (如果地點是城市 B，否則為 0）

修正後的回歸模型：

$$
Price=β_0+β_1⋅D_A+β_2⋅D_B+ϵ
$$

### 高共線性（High Multicollinearity）

當虛擬變量之間的相關性很高時，會導致多重共線性問題。解決方法包括使用變量選擇技術、正則化方法或合併高相關性的變量。

## 建立模型

### 5 methods of building models

1. All-in
2. Backward Elimination (向後逐步回歸)
3. Forward Selection (向前選取)
4. Bidirectional Elimination (雙向消除)
5. Score Comparison

### 1) All-in

是指在構建模型時將所有可用的特徵都包含在內，而不進行任何特徵選擇或特徵工程。

#### 特點

1. **直接使用所有特徵**
   - 不進行特徵選擇
   - 不進行特徵工程
2. **簡單快捷**
   - 不需要額外的特徵選擇步驟
   - 適合於初步探索數據集
3. **模型的複雜性**
   - 可能導致模型過於複雜
   - 模型的解釋性較差

#### 優點與缺點

**優點：**

- 簡單實現
- 避免特徵選擇錯誤
- 適合初步探索

**缺點：**

- 過擬合風險
- 模型複雜性高
- 解釋性差

#### 範例

假設我們有一個數據集，用於預測房屋價格，數據集中包含以下特徵：

- 面積（Square footage）
- 房間數量（Number of rooms）
- 房齡（Age of house）
- 距離市中心的距離（Distance from city center）
- 是否有花園（Has garden）
- 是否有車庫（Has garage）

在"all-in"方法中，我們不進行任何特徵選擇，直接使用所有這些特徵來訓練模型：

$$
Price = \beta_0 + \beta_1 ⋅ 面積 + \beta_2 ⋅ 房間數量 + \beta_3 ⋅ 房齡+ \beta_4 ⋅ 市中心距離 + \beta_5 ⋅ 有花園 + \beta_6 ⋅ 有車庫 + ϵ
$$

#### 結論

"All-in"方法是一種簡單直接的模型構建方式，適合於初步探索數據集和快速建立基線模型。然而，由於其可能導致過擬合和模型複雜性增加，後續的特徵選擇和模型優化仍然是必不可少的步驟。

### 2) Backward Elimination

是一種特徵選擇技術，通過逐步移除不顯著的特徵來簡化模型。

#### 步驟：

1. **建立完整模型**：使用所有特徵變量建立初始模型
2. **計算 P 值**
3. **檢查顯著性**：找到 p 值最大的變量。如果這個變量的 p 值大於設定的顯著性水平（例如 0.05），則將其從模型中移除
4. **重複步驟 2 和 3**：在移除變量後，重新擬合模型，並重新計算剩餘變量的 p 值。重複這個過程，直到所有剩餘變量的 p 值都小於設定的顯著性水平。
5. **完成**：最終模型僅包含顯著的特徵變量，這樣可以提高模型的簡潔性和解釋性。
6. **最終模型**：當所有剩餘變量的 p 值都小於設定的顯著性水平時，停止過程，最終得到的模型即為我們的最終模型

#### Note:

這種方法的優點在於它逐步簡化模型，使得最終模型更具解釋性。然而，它也有可能會移除一些在聯合作用下顯著的特徵，這是需要注意的潛在問題

#### 範例

假設我們有一個數據集，用來預測房屋價格，其中包含以下特徵：

- 面積（Square footage）
- 房間數量（Number of rooms）
- 房齡（Age of house）
- 距離市中心的距離（Distance from city center）
- 是否有花園（Has garden）
- 是否有車庫（Has garage）

我們希望使用這些變量來預測房屋價格（Price）。我們可以使用 Backward Elimination 來選擇最佳的特徵組合。

#### 步驟 1：建立初始模型

我們首先建立一個包含所有變量的回歸模型：

$$
Price = \beta_0 + \beta_1 ⋅ 面積 + \beta_2 ⋅ 房間數量 + \beta_3 ⋅ 房齡+ \beta_4 ⋅ 市中心距離 + \beta_5 ⋅ 有花園 + \beta_6 ⋅ 有車庫 + ϵ
$$

#### 步驟 2：計算 p 值

擬合模型後，計算每個變量的 p 值，假設結果如下：

- Square footage: 0.01
- Number of rooms: 0.04
- Age of house: 0.50
- Distance from city center: 0.02
- Has garden: 0.20
- Has garage: 0.15

#### 步驟 3：檢查顯著性

找到 p 值最大的變量（Age of house，p 值為 0.50），這個 p 值大於顯著性水平 0.05，因此我們將其從模型中移除。

p-value 為 0.05 ，表示虛無假設為真時，觀察到現有數據或比這更極端數據的概率是 5%。這並不是說這個事件不可能發生，而是說它相對罕見。

#### 步驟 4：重複步驟 2 和 3

在移除 Age of house 後，重新擬合模型並計算新的 p 值，假設結果如下：

- Square footage: 0.01
- Number of rooms: 0.04
- Distance from city center: 0.02
- Has garden: 0.20
- Has garage: 0.15

繼續移除 p 值最大的變量（Has garden，p 值為 0.20）。

再次擬合模型並計算新的 p 值，假設結果如下：

- Square footage: 0.01
- Number of rooms: 0.03
- Distance from city center: 0.02
- Has garage: 0.10

再次移除 p 值最大的變量（Has garage，p 值為 0.10）。

最終擬合模型並計算新的 p 值，假設結果如下：

- Square footage: 0.01
- Number of rooms: 0.03
- Distance from city center: 0.02

現在所有變量的 p 值都小於 0.05，因此我們停止移除變量，最終模型如下：

#### 最終模型

$$
Price = \beta_0 + \beta_1 ⋅ 面積 + \beta_2 ⋅ 房間數量 + \beta_3 ⋅ 市中心距離  + ϵ
$$

#### 優點與缺點

**優點**：

- 使模型更簡潔，更易於解釋。
- 可以提高模型的預測性能，特別是當存在多重共線性時。

**缺點**：

- 可能會忽略某些實際上重要但在某個步驟中顯得不顯著的變量。
- 依賴於 p 值的顯著性水平，可能會受到樣本大小和變量數量的影響

## 3) Forward Selection

Forward selection 是另一種特徵選擇技術，用於機器學習和統計模型中。與 backward elimination 相反，forward selection 是從一個空模型開始，逐步添加顯著的特徵，直到模型達到最佳性能或不再顯著改善。具體步驟如下

### 步驟：

1. **開始**：從一個沒有任何特徵變量的空模型開始。
2. **選擇最顯著的特徵**：對每個特徵變量進行評估，選擇對模型性能提高最顯著的特徵（通常使用某種統計測度，如 p 值、AIC、BIC 或其他評估指標），將其添加到模型中。
3. **重複添加特徵**：在每一步中，對尚未被選中的特徵進行評估，選擇最顯著的特徵並將其添加到模型中。
4. **停止條件**：重複步驟 2 和 3，直到沒有任何特徵的添加能顯著提高模型性能，或達到預先設定的特徵數量限制。
5. **完成**：最終模型包含了所有顯著的特徵變量，這樣可以提高模型的預測能力和解釋性。

這種方法的優點在於它逐步構建模型，使得最終模型包含最有意義的特徵，從而提高了模型的預測能力和解釋性。然而，這種方法可能會忽略一些在聯合作用下顯著的特徵，因為它是在每一步中僅考慮單個特徵的影響。

### Note:

簡而言之，forward selection 和 backward elimination 是兩種相對應的特徵選擇方法，一個從空模型開始逐步添加特徵，另一個從全特徵模型開始逐步移除特徵。兩者都旨在找到最優的特徵組合，以提高模型的性能和可解釋性

### 範例:

#### 步驟 1: 開始

從空模型開始，即模型中沒有任何特徵。

#### 步驟 2: 選擇最顯著的特徵

計算每個單獨特徵對目標變量 YYY 的影響，選擇最顯著的特徵。例如，使用線性回歸並計算每個特徵的 p 值：

- $X_1$ 的 p 值 = 0.02
- $X_2$ 的 p 值 = 0.15
- $X_3$ 的 p 值 = 0.01
- $X_4$ 的 p 值 = 0.05

在這裡， $X_3$ 的 p 值最小，因此我們將 $X_3$ 添加到模型中。

#### 步驟 3: 重複添加特徵

現在，我們的模型包含 $X_3$ 。接下來，我們評估剩餘特徵 $X_1$, $X_2$, $X_4$ 與 $X_3$ 的聯合作用。

對於每個剩餘特徵，計算其在包含 $X_3$ 的模型中的顯著性：

- $X_3 + X_1$ 的 p 值 = 0.03 (新增的 $X_1$ 的 p 值 = 0.03)
- $X_3 + X_2$ 的 p 值 = 0.10 (新增的 $X_2$ 的 p 值 = 0.10)
- $X_3 + X_4$ 的 p 值 = 0.04 (新增的 $X_4$ 的 p 值 = 0.04)

在這裡，新增的 $X_1$ 的 p 值最小，因此我們將 $X_1$ 添加到模型中。

#### 步驟 4: 繼續添加特徵

我們繼續這個過程，現在模型包含 $X_3$ 和 $X_1$ 。我們評估剩餘特徵 $X_2$ 和 $X_4$：

- $X_3+X_1+X_2$ 的 p 值 = 0.08 (新增的 $X_2$ 的 p 值 = 0.08)
- $X_3+X_1+X_4$ 的 p 值 = 0.02 (新增的 $X_4$ 的 p 值 = 0.02)

在這裡，新增的 $X_4$ 的 p 值最小，因此我們將 $X_4$ 添加到模型中。

#### 步驟 5: 停止條件

現在模型包含 $X_3, X_1, X_4$。我們評估最後一個特徵 $X_2$：

$X_3+X_1+X_4+X_2$ 的 p 值 = 0.15 (新增的 $X_2$ 的 p 值 = 0.15)

在這裡，新增的 $X_2$ 的 p 值較高，不顯著，因此我們不再添加 $X_2$

#### 完成

最終模型包含了 $X_3,X_1,X_4$ 三個顯著特徵。這個模型應該具有較好的預測能力和解釋性。

這個範例展示了 forward selection 的基本過程，即從空模型開始，逐步添加顯著的特徵，直到沒有任何特徵的添加能顯著提高模型性能

## 4) Bidirectional Elimination

Bidirectional elimination (雙向消除) 是一種結合了前向選擇 (forward selection) 和後向消除 (backward elimination) 的特徵選擇方法。這種方法在特徵選擇過程中同時進行特徵的添加和移除，以達到最優的模型。

### 步驟：

1. **初始化**：從空模型開始
2. **前向選擇**：添加最顯著特徵
3. **後向消除**：移除不顯著特徵
4. **重複步驟 2 和 3** :重複前向選擇和後向消除的過程，直到沒有任何特徵的添加或移除能顯著提高模型性能
5. **停止條件**：當無法再改進時停止

#### 範例

假設我們有一個包含四個特徵 $X_1, X_2, X_3, X_4$ 的數據集，目標變量為 $Y$。我們使用 bidirectional elimination 來選擇特徵。

##### 初始化

從空模型開始。

#### 第一步：前向選擇

計算每個單獨特徵對目標變量 $Y$ 的影響，選擇最顯著的特徵。例如：

- $X_1$ 的 p 值 = 0.02
- $X_2$ 的 p 值 = 0.15
- $X_3$ 的 p 值 = 0.01
- $X_4$ 的 p 值 = 0.05

在這裡，$X_3$ 的 p 值最小，因此我們將 $X_3$ 添加到模型中。

#### 第二步：後向消除

現在模型包含 $X_3$。沒有其他特徵，因此無需移除任何特徵。

#### 第三步：前向選擇

接下來，我們評估剩餘特徵 $X_1, X_2, X_4$ 與 $X_3$ 的聯合作用：

- $X_3 + X_1$ 的 p 值 = 0.03 (新增的 $X_1$ 的 p 值 = 0.03)
- $X_3 + X_2$ 的 p 值 = 0.10 (新增的 $X_2$ 的 p 值 = 0.10)
- $X_3 + X_4$ 的 p 值 = 0.04 (新增的 $X_4$ 的 p 值 = 0.04)

在這裡，新增的 $X_1$ 的 p 值最小，因此我們將 $X_1$ 添加到模型中。

#### 第四步：後向消除

現在模型包含 $X_3$ 和 $X_1$。我們重新評估這兩個特徵的顯著性：

- $X_3$ 的 p 值 = 0.02
- $X_1$ 的 p 值 = 0.03

兩個特徵都顯著，因此無需移除任何特徵。

#### 重複步驟

我們繼續這個過程，現在模型包含 $X_3$ 和 $X_1$。我們評估剩餘特徵 $X_2$ 和 $X_4$：

- $X_3+X_1+X_2$ 的 p 值 = 0.08 (新增的 $X_2$ 的 p 值 = 0.08)
- $X_3+X_1+X_4$ 的 p 值 = 0.02 (新增的 $X_4$ 的 p 值 = 0.02)

在這裡，新增的 $X_4$ 的 p 值最小，因此我們將 $X_4$ 添加到模型中。

現在模型包含 $X_3+X_1+X_4$。重新評估這些特徵的顯著性：

- $X_3$ 的 p 值 = 0.01
- $X_1$ 的 p 值 = 0.03
- $X_4$ 的 p 值 = 0.04

所有特徵都顯著，因此無需移除任何特徵。

#### 停止條件

最終模型包含了 $X_3, X_1, X_4$ 三個顯著特徵。所有剩餘特徵的 p 值都低於設定的閾值，且無法再添加或移除特徵以顯著提高模型性能，因此停止過程。

這個範例展示了 bidirectional elimination 的基本過程，即同時進行特徵的添加和移除，以達到最優的模型。這種方法能夠在前向選擇和後向消除之間取得平衡，從而選擇出對模型性能最有貢獻的特徵

## 特徵縮放

### 什麼是特徵縮放?

特徵縮放是將不同範圍的數值特徵轉換到相似範圍的過程。

### 為什麼需要特徵縮放？

1. **梯度下降法收斂速度**：幫助優化算法更快收斂
2. **數值穩定性**：避免數值計算問題
3. **特徵影響平衡**：確保各個特徵的影響力相當

### 示例

未縮放數據：

```bash
X = [500, 1000, 1500, 2000, 2500]
```

縮放後數據：

```bash
X_scaled = [0.1, 0.2, 0.3, 0.4, 0.5]
```

# Decision Tree Regression

## 目錄

- [Decision Tree Regression](#decision-tree-regression)
  - [目錄](#目錄)
  - [什麼是 Decision Tree Regression?](#什麼是-decision-tree-regression)
    - [數學表示](#數學表示)
    - [關鍵特點](#關鍵特點)
  - [應用場景](#應用場景)
  - [建立模型](#建立模型)
    - [步驟](#步驟)
  - [優點與缺點](#優點與缺點)
    - [優點](#優點)
    - [缺點](#缺點)
  - [注意事項](#注意事項)

## 什麼是 Decision Tree Regression?

Decision Tree Regression（決策樹回歸）是一種基於決策樹的回歸分析方法。它將數據集劃分為多個子集，每個子集對應一個決策樹節點，最終形成一個樹狀結構。每個節點代表一個特徵的閾值，通過遞歸地劃分數據，最終到達葉子節點，葉子節點的值即為預測值。

### 數學表示

決策樹回歸的目標是最小化每個區域內的方差：

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y})^2
$$

其中 $\hat{y}$ 是該區域內所有樣本的平均值。

### 關鍵特點

1. 不需要特徵縮放
2. 可以處理非線性關係
3. 模型具有良好的可解釋性

## 應用場景

1. **金融領域**

   - 股票價格預測
   - 風險評估
   - 信用評分

2. **房地產**

   - 房價預測
   - 租金評估

3. **醫療保健**

   - 病人住院時間預測
   - 醫療費用估算

4. **工業製造**
   - 產品質量預測
   - 設備壽命估計

## 建立模型

### 步驟

1. **數據準備**

   - 收集和清理數據
   - 分割訓練集和測試集

2. **模型訓練**

   ```python
   from sklearn.tree import DecisionTreeRegressor
   regressor = DecisionTreeRegressor(random_state=0)
   regressor.fit(X_train, y_train)
   ```

3. **參數調優**
   - max_depth: 樹的最大深度
   - min_samples_split: 分割內部節點所需的最小樣本數
   - min_samples_leaf: 葉節點所需的最小樣本數

## 優點與缺點

### 優點

1. **易於理解和解釋**

   - 決策過程透明
   - 可以輕易轉換為決策規則

2. **處理非線性關係**

   - 可以捕捉複雜的非線性模式
   - 不需要假設數據分佈

3. **特徵處理靈活**
   - 不需要特徵縮放
   - 可以處理數值和類別特徵

### 缺點

1. **容易過擬合**

   - 需要謹慎控制樹的深度
   - 可能需要剪枝處理

2. **不穩定性**

   - 對數據微小變化敏感
   - 可能產生完全不同的樹結構

3. **局部最優**
   - 每次分割只考慮局部最優
   - 可能錯過全局最優解

## 注意事項

1. **避免過擬合**

   - 使用交叉驗證
   - 設置適當的樹深度
   - 考慮使用集成方法（如隨機森林）

2. **特徵選擇**

   - 移除無關特徵
   - 注意特徵之間的相關性

3. **模型評估**
   - 使用多個評估指標
   - 考慮模型的穩定性
   - 進行殘差分析
